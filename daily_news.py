import os
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from openai import OpenAI

# ================= ğŸŒŸ ç»ˆææƒ…æŠ¥æºé…ç½® =================
rss_list = [
    # --- ğŸ‡¨ğŸ‡³ å›½å†…ä¸»åŠ› (ä½ çš„æœ€çˆ±) ---
    # é€‰ç”¨å®˜ç½‘æºï¼ˆå†…å®¹å’Œå…¬ä¼—å·ä¸€è‡´ï¼Œä½†æåº¦ç¨³å®šï¼Œä¸ä¼šæŠ¥é”™ï¼‰
    "https://www.jiqizhixin.com/rss",          # æœºå™¨ä¹‹å¿ƒ
    "https://www.qbitai.com/feed",             # é‡å­ä½
    "https://www.geekpark.net/rss",            # æå®¢å…¬å›­
    "https://feed.feeddd.org/feeds/Rockhazix", # æ•°å­—ç”Ÿå‘½å¡å…¹å…‹ (ä¸ªäººå·ï¼Œå¾ˆç¨³)

    # --- ğŸŒ æµ·å¤–å‰æ²¿ (è¡¥å……è§†é‡) ---
    # æ—¢ç„¶ä½ è¦åšæœ€é…·çš„åŠ©æ‰‹ï¼Œå¿…é¡»è¦æœ‰ç¡…è°·çš„ä¸€æ‰‹æ¶ˆæ¯
    "https://www.reddit.com/r/LocalLLaMA/top/.rss?t=day", # Reddit (å¼€æºå¤§æ¨¡å‹å¤§æœ¬è¥)
    "https://hnrss.org/newest?points=100",                # Hacker News (å…¨çƒæŠ€æœ¯çƒ­ç‚¹)
    "https://openai.com/blog/rss.xml",                    # OpenAI å®˜æ–¹åšå®¢
]

# AI è®¾ç½®
api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("OPENAI_API_BASE")
client = OpenAI(api_key=api_key, base_url=api_base)
# =======================================================

def get_rss_content(url):
    """
    å…¨èƒ½æŠ“å–å‡½æ•°ï¼š
    1. ä¼ªè£…æˆ Mac ç”µè„‘ä¸Šçš„ Chrome æµè§ˆå™¨ï¼ˆé˜²æ‹¦æˆªï¼‰ã€‚
    2. å…¼å®¹ RSS å’Œ Atom ä¸¤ç§æ ¼å¼ï¼ˆé˜²æŠ¥é”™ï¼‰ã€‚
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # è®¾ç½®è¶…æ—¶ï¼ŒReddit æœ‰æ—¶å€™æ…¢
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            print(f"âŒ æŠ“å–å¤±è´¥ {url}: {response.status_code}")
            return []
            
        content = response.text
        # å®¹é”™è§£æ
        try:
            root = ET.fromstring(content)
        except:
            root = ET.fromstring(content.encode('utf-8'))

        items = []
        # å‘½åç©ºé—´å¤„ç† (ç”¨äºè§£æå›½å¤– Atom æ ¼å¼)
        ns = {'atom': 'http://www.w3.org/2005/Atom'} 
        has_ns = 'http://www.w3.org/2005/Atom' in content
        
        # æ··åˆæŸ¥æ‰¾æ‰€æœ‰æ–‡ç« 
        entries = root.findall('.//item') + root.findall('.//atom:entry', ns if has_ns else {})
        
        # æ¯ä¸ªæºåªå–å‰ 6 æ¡ï¼Œé¿å…å†…å®¹å¤ªå¤šæ¶ˆåŒ–ä¸äº†
        for item in entries[:6]: 
            # æ ‡é¢˜
            title_node = item.find('title') if item.find('title') is not None else item.find('atom:title', ns if has_ns else {})
            title = title_node.text if title_node is not None else "æ— æ ‡é¢˜"
            
            # é“¾æ¥
            link = ""
            if item.find('link') is not None:
                link = item.find('link').text
            elif has_ns and item.find('atom:link', ns) is not None:
                link = item.find('atom:link', ns).attrib.get('href')
            
            # ç®€ä»‹ (ç”¨æ¥å¸® AI ç­›é€‰)
            desc = ""
            if item.find('description') is not None:
                desc = item.find('description').text
            elif has_ns and item.find('atom:summary', ns) is not None:
                desc = item.find('atom:summary', ns).text
                
            # ç®€å•æ¸…æ´— HTML æ ‡ç­¾
            if desc:
                desc = desc.replace('<p>', '').replace('</p>', '')[:200]

            items.append(f"ã€æ¥æºã€‘{url}\nã€æ ‡é¢˜ã€‘{title}\nã€ç®€ä»‹ã€‘{desc}\nã€é“¾æ¥ã€‘{link}\n")
            
        print(f"âœ… æˆåŠŸæŠ“å– {url}ï¼Œè·å– {len(items)} æ¡")
        return items
    except Exception as e:
        print(f"âš ï¸ æŠ“å–å‡ºé”™ {url}: {e}") # å‡ºé”™ä¸ä¸­æ–­ï¼Œç»§ç»­æŠ“ä¸‹ä¸€ä¸ª
        return []

def main():
    print("ğŸš€ å¼€å§‹å…¨ç½‘æ‰«æ (å›½å†…+å›½å¤–)...")
    all_news = []
    
    for url in rss_list:
        news_items = get_rss_content(url)
        all_news.extend(news_items)
    
    if not all_news:
        print("ğŸ“­ å±…ç„¶ä¸€æ¡æ–°é—»éƒ½æ²¡æŠ“åˆ°ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æºåœ°å€")
        return

    content_text = "\n\n".join(all_news)
    
    print("ğŸ¤– AI æ­£åœ¨é˜…è¯»ä¸­è‹±åŒè¯­ææ–™å¹¶æ’°å†™å‘¨æŠ¥...")

    # ğŸ”¥ æ ¸å¿ƒ Promptï¼šä¸­è¥¿åˆç’§ç‰ˆ
    prompt = f"""
    ä½ ç°åœ¨æ˜¯å…¨ç½‘æœ€æ‡‚ AI çš„â€œç§‘æŠ€æƒ…æŠ¥å®˜â€ã€‚ä½ çš„æ¡Œå­ä¸Šå †æ»¡äº†æ¥è‡ªã€æœºå™¨ä¹‹å¿ƒã€‘ã€ã€Redditã€‘ã€ã€OpenAIã€‘çš„æœ€æ–°æƒ…æŠ¥ã€‚
    
    è¯·æŠŠè¿™äº›ä¸­è‹±æ–‡æ··æ‚çš„å†…å®¹ï¼Œæ•´ç†æˆä¸€ä»½**â€œä»Šæ—¥ AI å¿…è¯»â€**ã€‚

    ### ä½ çš„ä»»åŠ¡ï¼š
    1.  **ç­›é€‰**ï¼šæŒ‘å‡º **6-8 æ¡** çœŸæ­£é‡è¦çš„æ–°é—»ã€‚
        -   å¦‚æœæœ‰**å›½å†…**çš„å¤§æ¨¡å‹/å¤§å‚åŠ¨æ€ï¼Œå¿…é¡»ä¿ç•™ã€‚
        -   å¦‚æœæœ‰**å›½å¤–**çš„å¼€æº/æŠ€æœ¯çªç ´ (æ¥è‡ª Reddit/HN)ï¼Œå¿…é¡»ä¿ç•™å¹¶**ç¿»è¯‘æˆä¸­æ–‡**ã€‚
    2.  **é£æ ¼**ï¼š
        -   æ ‡é¢˜è¦åƒâ€œå…¬ä¼—å·çˆ†æ¬¾æ–‡â€ï¼Œå¸¦ Emojiï¼Œå¸å¼•äººç‚¹å‡»ã€‚
        -   å†…å®¹è¦â€œè¯´äººè¯â€ï¼Œä¸è¦æ¯ç‡¥çš„ç¿»è¯‘è…”ã€‚å¦‚æœå›½å¤–æ–°é—»æ¯”è¾ƒç¡¬æ ¸ï¼Œè¯·ç”¨é€šä¿—çš„è¯­è¨€è§£é‡Šä¸€ä¸‹â€œè¿™æœ‰ä»€ä¹ˆç”¨â€ã€‚
    3.  **æ ¼å¼**ï¼š
        -   **[ğŸŒ å…¨çƒè§†é‡]** (æ”¾å›½å¤–é‡ç£…)
        -   **[ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€]** (æ”¾æœºå™¨ä¹‹å¿ƒ/é‡å­ä½çš„å†…å®¹)
        -   **[ğŸ› ï¸ å¼€å‘è€…/å·¥å…·]** (æ–°å‡ºçš„å¥½ç©å·¥å…·)
        -   æ¯æ¡æ–°é—»æœ€åéƒ½è¦é™„ä¸Š [ğŸ”—åŸæ–‡é“¾æ¥]ã€‚

    ### è¾“å…¥ç´ æï¼š
    {content_text}
    """

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=1.0 # ä¿æŒé«˜åˆ›é€ æ€§
    )
    
    print("="*30)
    print(response.choices[0].message.content)
    print("="*30)

if __name__ == "__main__":
    main()
